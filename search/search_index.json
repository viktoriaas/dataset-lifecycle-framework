{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dataset Lifecycle Framework Our Framework introduces the Dataset CRD which is a pointer to existing S3 and NFS data sources. It includes the necessary logic to map these Datasets into Persistent Volume Claims and ConfigMaps which users can reference in their pods, letting them focus on the workload development and not on configuring/mounting/tuning the data access. Thanks to Container Storage Interface it is extensible to support additional data sources in the future. A Kubernetes Framework to provide easy access to S3 and NFS Datasets within pods. Orchestrates the provisioning of Persistent Volume Claims and ConfigMaps needed for each Dataset . Find more details in our FAQ Quickstart In order to quickly deploy DLF, based on your environment execute one of the following commands: Kubernetes/Minikube kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf.yaml Kubernetes on IBM Cloud kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-ibm-k8s.yaml Openshift kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-oc.yaml Openshift on IBM Cloud kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-ibm-oc.yaml Wait for all the pods to be ready :) kubectl wait --for = condition = ready pods -l app.kubernetes.io/name = dlf -n dlf As an optional step, label the namespace you want to have the pods labelling functionality (see below) kubectl label namespace default monitor-pods-datasets = enabled In case don't have an existing S3 Bucket follow our wiki to deploy an Object Store and populate it with data. We will create now a Dataset named example-dataset pointing to your S3 bucket. cat <<EOF | kubectl apply -f - apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : example-dataset spec : local : type : \"COS\" accessKeyID : \"{AWS_ACCESS_KEY_ID}\" secretAccessKey : \"{AWS_SECRET_ACCESS_KEY}\" endpoint : \"{S3_SERVICE_URL}\" bucket : \"{BUCKET_NAME}\" readonly : \"true\" #OPTIONAL, default is false region : \"\" #OPTIONAL EOF If everything worked okay, you should see a PVC and a ConfigMap named example-dataset which you can mount in your pods. As an easier way to use the Dataset in your pod, you can instead label the pod as follows: apiVersion : v1 kind : Pod metadata : name : nginx labels : dataset.0.id : \"example-dataset\" dataset.0.useas : \"mount\" spec : containers : - name : nginx image : nginx As a convention the Dataset will be mounted in /mnt/datasets/example-dataset . If instead you wish to pass the connection details as environment variables, change the useas line to dataset.0.useas: \"configmap\" Feel free to explore our examples Roadmap Have a look on our wiki for Roadmap Contact Reach out to us via email: - Yiannis Gkoufas, yiannisg@ie.ibm.com - Christian Pinto, christian.pinto@ibm.com - Srikumar Venugopal, srikumarv@ie.ibm.com Acknowledgements This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 825061. H2020 evolve .","title":"Introduction"},{"location":"#dataset-lifecycle-framework","text":"Our Framework introduces the Dataset CRD which is a pointer to existing S3 and NFS data sources. It includes the necessary logic to map these Datasets into Persistent Volume Claims and ConfigMaps which users can reference in their pods, letting them focus on the workload development and not on configuring/mounting/tuning the data access. Thanks to Container Storage Interface it is extensible to support additional data sources in the future. A Kubernetes Framework to provide easy access to S3 and NFS Datasets within pods. Orchestrates the provisioning of Persistent Volume Claims and ConfigMaps needed for each Dataset . Find more details in our FAQ","title":"Dataset Lifecycle Framework"},{"location":"#quickstart","text":"In order to quickly deploy DLF, based on your environment execute one of the following commands: Kubernetes/Minikube kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf.yaml Kubernetes on IBM Cloud kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-ibm-k8s.yaml Openshift kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-oc.yaml Openshift on IBM Cloud kubectl apply -f https://raw.githubusercontent.com/IBM/dataset-lifecycle-framework/master/release-tools/manifests/dlf-ibm-oc.yaml Wait for all the pods to be ready :) kubectl wait --for = condition = ready pods -l app.kubernetes.io/name = dlf -n dlf As an optional step, label the namespace you want to have the pods labelling functionality (see below) kubectl label namespace default monitor-pods-datasets = enabled In case don't have an existing S3 Bucket follow our wiki to deploy an Object Store and populate it with data. We will create now a Dataset named example-dataset pointing to your S3 bucket. cat <<EOF | kubectl apply -f - apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : example-dataset spec : local : type : \"COS\" accessKeyID : \"{AWS_ACCESS_KEY_ID}\" secretAccessKey : \"{AWS_SECRET_ACCESS_KEY}\" endpoint : \"{S3_SERVICE_URL}\" bucket : \"{BUCKET_NAME}\" readonly : \"true\" #OPTIONAL, default is false region : \"\" #OPTIONAL EOF If everything worked okay, you should see a PVC and a ConfigMap named example-dataset which you can mount in your pods. As an easier way to use the Dataset in your pod, you can instead label the pod as follows: apiVersion : v1 kind : Pod metadata : name : nginx labels : dataset.0.id : \"example-dataset\" dataset.0.useas : \"mount\" spec : containers : - name : nginx image : nginx As a convention the Dataset will be mounted in /mnt/datasets/example-dataset . If instead you wish to pass the connection details as environment variables, change the useas line to dataset.0.useas: \"configmap\" Feel free to explore our examples","title":"Quickstart"},{"location":"#roadmap","text":"Have a look on our wiki for Roadmap","title":"Roadmap"},{"location":"#contact","text":"Reach out to us via email: - Yiannis Gkoufas, yiannisg@ie.ibm.com - Christian Pinto, christian.pinto@ibm.com - Srikumar Venugopal, srikumarv@ie.ibm.com","title":"Contact"},{"location":"#acknowledgements","text":"This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 825061. H2020 evolve .","title":"Acknowledgements"},{"location":"Archive-based-Datasets/","text":"Prerequisites We will work with the branch archive You have kubectl utility installed and your account has admin rights to install service accounts etc For demo purposes you can use minikube. Installation git clone https://github.com/IBM/dataset-lifecycle-framework.git cd dataset-lifecycle-framework git checkout archive After you check out the project and the correct branch, proceed with the installation of minio. If you already have a cloud object store, you can skip this step. make deployment kubectl apply -n dlf -f examples/minio/ The above will install on the components in the dlf namespace. A final step would be to create a secret named minio-conf in the dlf namespace which would point on the connection information for the cloud object store you would be using. In the case you have provisioned our demo minio instance, execute the below. In different case adopt the connection details to reflect on your setup. kubectl create secret generic minio-conf --from-literal = 'AWS_ACCESS_KEY_ID=minio' --from-literal = 'AWS_SECRET_ACCESS_KEY=minio123' --from-literal = 'ENDPOINT=http://minio-service:9000' -n dlf You can check the status of the installation: watch kubectl get pods -n dlf When all the components are ready the output should look like this: NAME READY STATUS RESTARTS AGE csi-attacher-nfsplugin-0 2 /2 Running 0 3m1s csi-attacher-s3-0 1 /1 Running 0 3m1s csi-hostpath-attacher-0 1 /1 Running 0 3m1s csi-hostpath-provisioner-0 1 /1 Running 0 3m1s csi-hostpathplugin-0 3 /3 Running 0 3m1s csi-nodeplugin-nfsplugin-vs7d9 2 /2 Running 0 3m1s csi-provisioner-s3-0 1 /1 Running 0 3m1s csi-s3-mrndx 2 /2 Running 0 3m1s dataset-operator-76798546cf-9d6wj 1 /1 Running 0 3m1s generate-keys-n7m5l 0 /1 Completed 0 3m1s minio-7979c89d5c-khncd 0 /1 Running 0 3m Usage Now we can create a Dataset based on a remote archive as follows: cat <<EOF | kubectl apply -f - apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : example-dataset spec : type : \"ARCHIVE\" url : \"https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz\" format : \"application/x-tar\" EOF You should see now a PVC created with the same name: $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE example-dataset Bound pvc-c58852a6-a597-4eb8-a05b-23d9899226bf 9314Gi RWX csi-s3 15s You can reference the dataset in the pod either as a usual PVC or by using the labels as follows: cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Pod metadata : name : nginx labels : dataset.0.id : \"example-dataset\" dataset.0.useas : \"mount\" spec : containers : - name : nginx image : nginx EOF You can exec into the pod (after it has started) and inspect that the Dataset is available as follows: $ kubectl exec -it nginx /bin/bash root@nginx:/# ls /mnt/datasets/example-dataset/ noaa-weather-data-jfk-airport root@nginx:/# ls /mnt/datasets/example-dataset/noaa-weather-data-jfk-airport/ LICENSE.txt README.txt clean_data.py jfk_weather.csv jfk_weather_cleaned.csv","title":"Archive-based Datasets"},{"location":"Archive-based-Datasets/#prerequisites","text":"We will work with the branch archive You have kubectl utility installed and your account has admin rights to install service accounts etc For demo purposes you can use minikube.","title":"Prerequisites"},{"location":"Archive-based-Datasets/#installation","text":"git clone https://github.com/IBM/dataset-lifecycle-framework.git cd dataset-lifecycle-framework git checkout archive After you check out the project and the correct branch, proceed with the installation of minio. If you already have a cloud object store, you can skip this step. make deployment kubectl apply -n dlf -f examples/minio/ The above will install on the components in the dlf namespace. A final step would be to create a secret named minio-conf in the dlf namespace which would point on the connection information for the cloud object store you would be using. In the case you have provisioned our demo minio instance, execute the below. In different case adopt the connection details to reflect on your setup. kubectl create secret generic minio-conf --from-literal = 'AWS_ACCESS_KEY_ID=minio' --from-literal = 'AWS_SECRET_ACCESS_KEY=minio123' --from-literal = 'ENDPOINT=http://minio-service:9000' -n dlf You can check the status of the installation: watch kubectl get pods -n dlf When all the components are ready the output should look like this: NAME READY STATUS RESTARTS AGE csi-attacher-nfsplugin-0 2 /2 Running 0 3m1s csi-attacher-s3-0 1 /1 Running 0 3m1s csi-hostpath-attacher-0 1 /1 Running 0 3m1s csi-hostpath-provisioner-0 1 /1 Running 0 3m1s csi-hostpathplugin-0 3 /3 Running 0 3m1s csi-nodeplugin-nfsplugin-vs7d9 2 /2 Running 0 3m1s csi-provisioner-s3-0 1 /1 Running 0 3m1s csi-s3-mrndx 2 /2 Running 0 3m1s dataset-operator-76798546cf-9d6wj 1 /1 Running 0 3m1s generate-keys-n7m5l 0 /1 Completed 0 3m1s minio-7979c89d5c-khncd 0 /1 Running 0 3m","title":"Installation"},{"location":"Archive-based-Datasets/#usage","text":"Now we can create a Dataset based on a remote archive as follows: cat <<EOF | kubectl apply -f - apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : example-dataset spec : type : \"ARCHIVE\" url : \"https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz\" format : \"application/x-tar\" EOF You should see now a PVC created with the same name: $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE example-dataset Bound pvc-c58852a6-a597-4eb8-a05b-23d9899226bf 9314Gi RWX csi-s3 15s You can reference the dataset in the pod either as a usual PVC or by using the labels as follows: cat <<EOF | kubectl apply -f - apiVersion : v1 kind : Pod metadata : name : nginx labels : dataset.0.id : \"example-dataset\" dataset.0.useas : \"mount\" spec : containers : - name : nginx image : nginx EOF You can exec into the pod (after it has started) and inspect that the Dataset is available as follows: $ kubectl exec -it nginx /bin/bash root@nginx:/# ls /mnt/datasets/example-dataset/ noaa-weather-data-jfk-airport root@nginx:/# ls /mnt/datasets/example-dataset/noaa-weather-data-jfk-airport/ LICENSE.txt README.txt clean_data.py jfk_weather.csv jfk_weather_cleaned.csv","title":"Usage"},{"location":"Ceph-Caching/","text":"The instructions currently work only for the branch https://github.com/IBM/dataset-lifecycle-framework/tree/fixed-caching Minikube installation First we need to have a working cluster. minikube start --memory='10G' --cpus=8 --disk-size='15g' --driver=docker Rook/Ceph Installation We need to have ceph installed. Inside plugins/ceph-cache-plugin/deploy/rook directory execute: kubectl create -f common.yaml Inspect the file keys-installation.sh and make sure you replace YOUR_REGISTRY,YOUR_EMAIL,YOUR_PASSWORD with the correct values for your docker registry and execute: ./keys-installation.sh Inspect the file operator.yaml and replace the value YOUR_REGISTRY and execute: kubectl create -f operator.yaml Inspect the file cluster.yaml and replace the value YOUR_REGISTRY and execute: kubectl create -f cluster.yaml If everything worked correctly the pods in the rook-ceph namespace should be like this: rook-ceph-mgr-a-5f8f5c978-xgcpw 1 /1 Running 0 79s rook-ceph-mon-a-6879b87656-bxbrw 1 /1 Running 0 89s rook-ceph-operator-86f9b59b8-2fvkb 1 /1 Running 0 5m29s rook-ceph-osd-0-9dcb46c48-hrzvz 1 /1 Running 0 43s NOTE If you want to delete/create a new cluster, besides invoking kubectl delete -f cluster.yaml You need also to delete the paths in defined in dataDirHostPath and directories.path Now we can proceed with installing DLF. DLF Installation Go into the root of this directory and execute: make deployment The pods in the default namespace would look like this: csi-attacher-nfsplugin-0 2 /2 Running 0 7s csi-attacher-s3-0 1 /1 Running 0 8s csi-nodeplugin-nfsplugin-nqgtl 2 /2 Running 0 7s csi-provisioner-s3-0 2 /2 Running 0 8s csi-s3-k9b5j 2 /2 Running 0 8s dataset-operator-7b8f65f7d4-hg8n5 1 /1 Running 0 6s Create an s3 dataset by replacing the values and invoking kubectl create -f my-dataset.yaml apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : example-dataset spec : local : type : \"COS\" accessKeyID : \"{AWS_ACCESS_KEY_ID}\" secretAccessKey : \"{AWS_SECRET_ACCESS_KEY}\" endpoint : \"{S3_SERVICE_URL}\" bucket : \"{BUCKET_NAME}\" region : \"\" #it can be empty Now if you check about datasetsinternal and PVC you would be able to see the example-dataset kubectl get datasetsinternal kubectl get pvc Delete the dataset we created before by executing kubectl delete dataset/example-dataset If you execute kubectl describe datasetinternal/example-dataset you would see the credentials and the endpoints you originally specified. Let's try to add the caching plugin. Ceph Caching Plugin Installation Change into the directory and invoke: make deployment Let's create the same dataset now that the plugin is deployed: kubectl create -f my-dataset.yaml You should see a new rgw pod starting up on rook-ceph namespace: rook-ceph-rgw-test-a-77f78b7b69-z5kp9 1 /1 Running 0 4m43s After a couple of minutes if you list datasetsinternal you will see the example-dataset created. If you describe it using kubectl describe datasetinternal/example-dataset you will notice that the credentials are different and they point to the rados gateway instance, therefore the PVC would reflect the cached version of the dataset.","title":"Ceph Caching"},{"location":"Ceph-Caching/#minikube-installation","text":"First we need to have a working cluster. minikube start --memory='10G' --cpus=8 --disk-size='15g' --driver=docker","title":"Minikube installation"},{"location":"Ceph-Caching/#rookceph-installation","text":"We need to have ceph installed. Inside plugins/ceph-cache-plugin/deploy/rook directory execute: kubectl create -f common.yaml Inspect the file keys-installation.sh and make sure you replace YOUR_REGISTRY,YOUR_EMAIL,YOUR_PASSWORD with the correct values for your docker registry and execute: ./keys-installation.sh Inspect the file operator.yaml and replace the value YOUR_REGISTRY and execute: kubectl create -f operator.yaml Inspect the file cluster.yaml and replace the value YOUR_REGISTRY and execute: kubectl create -f cluster.yaml If everything worked correctly the pods in the rook-ceph namespace should be like this: rook-ceph-mgr-a-5f8f5c978-xgcpw 1 /1 Running 0 79s rook-ceph-mon-a-6879b87656-bxbrw 1 /1 Running 0 89s rook-ceph-operator-86f9b59b8-2fvkb 1 /1 Running 0 5m29s rook-ceph-osd-0-9dcb46c48-hrzvz 1 /1 Running 0 43s NOTE If you want to delete/create a new cluster, besides invoking kubectl delete -f cluster.yaml You need also to delete the paths in defined in dataDirHostPath and directories.path Now we can proceed with installing DLF.","title":"Rook/Ceph Installation"},{"location":"Ceph-Caching/#dlf-installation","text":"Go into the root of this directory and execute: make deployment The pods in the default namespace would look like this: csi-attacher-nfsplugin-0 2 /2 Running 0 7s csi-attacher-s3-0 1 /1 Running 0 8s csi-nodeplugin-nfsplugin-nqgtl 2 /2 Running 0 7s csi-provisioner-s3-0 2 /2 Running 0 8s csi-s3-k9b5j 2 /2 Running 0 8s dataset-operator-7b8f65f7d4-hg8n5 1 /1 Running 0 6s Create an s3 dataset by replacing the values and invoking kubectl create -f my-dataset.yaml apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : example-dataset spec : local : type : \"COS\" accessKeyID : \"{AWS_ACCESS_KEY_ID}\" secretAccessKey : \"{AWS_SECRET_ACCESS_KEY}\" endpoint : \"{S3_SERVICE_URL}\" bucket : \"{BUCKET_NAME}\" region : \"\" #it can be empty Now if you check about datasetsinternal and PVC you would be able to see the example-dataset kubectl get datasetsinternal kubectl get pvc Delete the dataset we created before by executing kubectl delete dataset/example-dataset If you execute kubectl describe datasetinternal/example-dataset you would see the credentials and the endpoints you originally specified. Let's try to add the caching plugin.","title":"DLF Installation"},{"location":"Ceph-Caching/#ceph-caching-plugin-installation","text":"Change into the directory and invoke: make deployment Let's create the same dataset now that the plugin is deployed: kubectl create -f my-dataset.yaml You should see a new rgw pod starting up on rook-ceph namespace: rook-ceph-rgw-test-a-77f78b7b69-z5kp9 1 /1 Running 0 4m43s After a couple of minutes if you list datasetsinternal you will see the example-dataset created. If you describe it using kubectl describe datasetinternal/example-dataset you will notice that the credentials are different and they point to the rados gateway instance, therefore the PVC would reflect the cached version of the dataset.","title":"Ceph Caching Plugin Installation"},{"location":"FAQ/","text":"What is the framework offering exactly? One new Custom Resource Definition: the Dataset. Essentially this CRD is a declarative way to reference an existing data source. Moreover, we provide a mount-point in user's pod for each Dataset and expose an interface for caching mechanisms to leverage. Current implementation supports S3- and NFS-based data sources. That's it? You just add one more CRD? Not quite. For every Dataset we create one Persistent Volume Claim which users can mount directly to their pods. We have implemented that logic as a regular Kubernetes Operator. What is the motivation for this work? What problem does it solve? Since the introduction of Container Storage Interface, there are more and more storage providers becoming available on Kubernetes environments. However we feel that for the non-experienced Kubernetes users it might be a high barrier for them to install/maintain/configure in order to leverage the available CSI plugins and gain access to the remote data sources on their pods. By introducing a higher level of abstraction (Dataset) and by taking care of all the necessary work around invoking the appropriate CSI plugin, configuring and provisioning the PVC we aim to improve the User Experience of data access in Kubernetes So...you want to replace CSI? On the contrary! Every type of data source we support actually comes with its own completely standalone CSI implementation. We are aspiring to be a meta-framework for the CSI plugins . If we have to make a comparison, we want make accessible different types of data sources the same way Kubeflow makes Machine Learning frameworks accessible on Kubernetes Are you competing with the COSI proposal? Absolutely no . When we started the project the only S3-based CSI plugin we were aware of was https://github.com/ctrox/csi-s3 which we have forked and maintain in our repo https://github.com/IBM/dataset-lifecycle-framework/tree/master/src/csi-s3 When the COSI interface becomes part of Kubernetes we will stop maintaining and directly support COSI for creating a PVC for buckets in Object Stores . COSI aims to manage the full lifecycle of a bucket like provisioning, configuring access etc. which is beyond our scope. Any other potential benefits you see with the framework? We believe that by introducing Dataset as a CRD you can accomplish higher level orchestration and bring contributions on: - Performance : We have attempted to create a pluggable caching interface like the example implementation: Ceph Caching Plugin - Security : Another effort we are exploring is to have a common access management layer for credentials of the different types of datasources Is anyone actually interested in the framework? European Bioinformatics Institute ( https://www.ebi.ac.uk/ ) are running a POC with DLF and Kubeflow on their cloud infrastructure David Yu Yuan actually reached out to us after a CNCF presentation People from Open Data Hub ( https://opendatahub.io/ ) are interested in integrating DLF in ODH See relevant issue ( https://github.com/IBM/dataset-lifecycle-framework/issues/40 ) Pachyderm's proposal is actually very close to the Dataset spec we are supporting. DLF is forked in their repo and is under evaluation in their repo https://github.com/pachyderm/kfdata","title":"FAQ"},{"location":"FAQ/#what-is-the-framework-offering-exactly","text":"One new Custom Resource Definition: the Dataset. Essentially this CRD is a declarative way to reference an existing data source. Moreover, we provide a mount-point in user's pod for each Dataset and expose an interface for caching mechanisms to leverage. Current implementation supports S3- and NFS-based data sources.","title":"What is the framework offering exactly?"},{"location":"FAQ/#thats-it-you-just-add-one-more-crd","text":"Not quite. For every Dataset we create one Persistent Volume Claim which users can mount directly to their pods. We have implemented that logic as a regular Kubernetes Operator.","title":"That's it? You just add one more CRD?"},{"location":"FAQ/#what-is-the-motivation-for-this-work-what-problem-does-it-solve","text":"Since the introduction of Container Storage Interface, there are more and more storage providers becoming available on Kubernetes environments. However we feel that for the non-experienced Kubernetes users it might be a high barrier for them to install/maintain/configure in order to leverage the available CSI plugins and gain access to the remote data sources on their pods. By introducing a higher level of abstraction (Dataset) and by taking care of all the necessary work around invoking the appropriate CSI plugin, configuring and provisioning the PVC we aim to improve the User Experience of data access in Kubernetes","title":"What is the motivation for this work? What problem does it solve?"},{"location":"FAQ/#soyou-want-to-replace-csi","text":"On the contrary! Every type of data source we support actually comes with its own completely standalone CSI implementation. We are aspiring to be a meta-framework for the CSI plugins . If we have to make a comparison, we want make accessible different types of data sources the same way Kubeflow makes Machine Learning frameworks accessible on Kubernetes","title":"So...you want to replace CSI?"},{"location":"FAQ/#are-you-competing-with-the-cosi-proposal","text":"Absolutely no . When we started the project the only S3-based CSI plugin we were aware of was https://github.com/ctrox/csi-s3 which we have forked and maintain in our repo https://github.com/IBM/dataset-lifecycle-framework/tree/master/src/csi-s3 When the COSI interface becomes part of Kubernetes we will stop maintaining and directly support COSI for creating a PVC for buckets in Object Stores . COSI aims to manage the full lifecycle of a bucket like provisioning, configuring access etc. which is beyond our scope.","title":"Are you competing with the COSI proposal?"},{"location":"FAQ/#any-other-potential-benefits-you-see-with-the-framework","text":"We believe that by introducing Dataset as a CRD you can accomplish higher level orchestration and bring contributions on: - Performance : We have attempted to create a pluggable caching interface like the example implementation: Ceph Caching Plugin - Security : Another effort we are exploring is to have a common access management layer for credentials of the different types of datasources","title":"Any other potential benefits you see with the framework?"},{"location":"FAQ/#is-anyone-actually-interested-in-the-framework","text":"European Bioinformatics Institute ( https://www.ebi.ac.uk/ ) are running a POC with DLF and Kubeflow on their cloud infrastructure David Yu Yuan actually reached out to us after a CNCF presentation People from Open Data Hub ( https://opendatahub.io/ ) are interested in integrating DLF in ODH See relevant issue ( https://github.com/IBM/dataset-lifecycle-framework/issues/40 ) Pachyderm's proposal is actually very close to the Dataset spec we are supporting. DLF is forked in their repo and is under evaluation in their repo https://github.com/pachyderm/kfdata","title":"Is anyone actually interested in the framework?"},{"location":"Roadmap/","text":"The order of the features/milestones represents loosely the order of which development will start. Noobaa Caching Plugin The S3-to-S3 caching is currently only supported by the Ceph/Rook-based plugin. However, we have been facing various problems as it's setup/configuration is not fully dynamic the way Noobaa is. In the wiki Caching-Remote-Buckets-(User-Guide) we have few hints about how to provision the cache buckets and this logic would be reflected on the Noobaa Caching Plugin Object Bucket API Our current approach is based on our modified version of csi-s3 which is not maintained. The Object Bucket API will reduce the code we have to maintain as the S3 operations would be supported in a more K8s native manner with the new API. All the S3-related operations should be replaced with the Object Bucket API once it's ready to be used. Vault-based access management In our current approach, for the datasets which require credentials are stored in secrets. Secrets is the de-facto kubernetes solution for storing credentials. However there are some problems when it comes to datasets. We might want to restrict the access to the datasets between the users in the same namespace. We would be able to support scenarios where UserA and UserB are on the same namespace but UserA has datasets which only they can access. Plan to leverage TSI Spectrum Scale Caching Plugin Assuming Spectrum Scale installed on hosts we could leverage ibm-spectrum-scale-csi to provide the same functionality of S3 caching as Ceph-based and Noobaa-based. Dataset Eviction from cache In our current approach, in the one implementation we have of a caching plugin, every dataset is being cached without priorities or checks (whether the cache is full etc). We need to tackle this. The most naive way to solve it is to not to use cache for a newly created dataset when the cache is full. A more sophisticated approach would be to monitor the usage of datasets and decide to evict based on some configurable policies. Sequential Transformation of Datasets In our current approach, the only possible transformation we have is Dataset -> DatasetInternal -> PVCs. In the future we would like to be able to support any number of transformation of any type. So there would be plugins that can handle a flow like this: Dataset(s3) -(caching)-> DatasetInternal(s3) -(expose)-> DatasetInternal(NFS) -> PVC That would give the users the capability to cache and export their datasets in the format of their preference. Simple Scheduling Hints Since we are aware of the nodes where a dataset is cached we can potentially offer this information to external schedulers or decorate the pods using nodeAffinity to assist the default Kubernetes scheduler to place the pods closer to the cached data. This is expected to improve the performance of the pods using the specific datasets.","title":"Roadmap"},{"location":"Roadmap/#noobaa-caching-plugin","text":"The S3-to-S3 caching is currently only supported by the Ceph/Rook-based plugin. However, we have been facing various problems as it's setup/configuration is not fully dynamic the way Noobaa is. In the wiki Caching-Remote-Buckets-(User-Guide) we have few hints about how to provision the cache buckets and this logic would be reflected on the Noobaa Caching Plugin","title":"Noobaa Caching Plugin"},{"location":"Roadmap/#object-bucket-api","text":"Our current approach is based on our modified version of csi-s3 which is not maintained. The Object Bucket API will reduce the code we have to maintain as the S3 operations would be supported in a more K8s native manner with the new API. All the S3-related operations should be replaced with the Object Bucket API once it's ready to be used.","title":"Object Bucket API"},{"location":"Roadmap/#vault-based-access-management","text":"In our current approach, for the datasets which require credentials are stored in secrets. Secrets is the de-facto kubernetes solution for storing credentials. However there are some problems when it comes to datasets. We might want to restrict the access to the datasets between the users in the same namespace. We would be able to support scenarios where UserA and UserB are on the same namespace but UserA has datasets which only they can access. Plan to leverage TSI","title":"Vault-based access management"},{"location":"Roadmap/#spectrum-scale-caching-plugin","text":"Assuming Spectrum Scale installed on hosts we could leverage ibm-spectrum-scale-csi to provide the same functionality of S3 caching as Ceph-based and Noobaa-based.","title":"Spectrum Scale Caching Plugin"},{"location":"Roadmap/#dataset-eviction-from-cache","text":"In our current approach, in the one implementation we have of a caching plugin, every dataset is being cached without priorities or checks (whether the cache is full etc). We need to tackle this. The most naive way to solve it is to not to use cache for a newly created dataset when the cache is full. A more sophisticated approach would be to monitor the usage of datasets and decide to evict based on some configurable policies.","title":"Dataset Eviction from cache"},{"location":"Roadmap/#sequential-transformation-of-datasets","text":"In our current approach, the only possible transformation we have is Dataset -> DatasetInternal -> PVCs. In the future we would like to be able to support any number of transformation of any type. So there would be plugins that can handle a flow like this: Dataset(s3) -(caching)-> DatasetInternal(s3) -(expose)-> DatasetInternal(NFS) -> PVC That would give the users the capability to cache and export their datasets in the format of their preference.","title":"Sequential Transformation of Datasets"},{"location":"Roadmap/#simple-scheduling-hints","text":"Since we are aware of the nodes where a dataset is cached we can potentially offer this information to external schedulers or decorate the pods using nodeAffinity to assist the default Kubernetes scheduler to place the pods closer to the cached data. This is expected to improve the performance of the pods using the specific datasets.","title":"Simple Scheduling Hints"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/","text":"We will show how you can use DLF to provision Data Volumes for your notebook servers. This would be helpful in the cases your training data are stored in S3 Buckets. Requirements You have access to the kubeflow dashboard and you have DLF installed. Make sure you first follow the guide for Installation Create a Dataset for the S3 Bucket In this guide, we assume that your data are already stored in a remote s3 bucket. Let's assume that you will launch your notebook server on the namespace {my-namespace} apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : your-dataset spec : local : type : \"COS\" accessKeyID : \"access_key_id\" secretAccessKey : \"secret_access_key\" endpoint : \"https://YOUR_ENDPOINT\" bucket : \"YOUR_BUCKET\" region : \"\" #it can be empty Now just execute: kubectl create -f my-dataset.yaml -n { my-namespace } Provision Notebook with the Data Volume Now use the Kubeflow Central Dashboard to follow the rest of the guide. Choose the \"Notebook Servers\" item: Select \"New server\": Head over to the \"Data Volumes\" section and fill out the form as follows: Now you can press \"Launch\" to start the notebook server. After you connect, you can list the contents of /mnt/dataset and verity that the reflect the contents for your remote S3 bucket. NOTE : all the changes that you do in this directory (delete,create,modify) will be reflected on the remote bucket","title":"Data Volumes for Notebook Servers"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#requirements","text":"You have access to the kubeflow dashboard and you have DLF installed. Make sure you first follow the guide for Installation","title":"Requirements"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#create-a-dataset-for-the-s3-bucket","text":"In this guide, we assume that your data are already stored in a remote s3 bucket. Let's assume that you will launch your notebook server on the namespace {my-namespace} apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : your-dataset spec : local : type : \"COS\" accessKeyID : \"access_key_id\" secretAccessKey : \"secret_access_key\" endpoint : \"https://YOUR_ENDPOINT\" bucket : \"YOUR_BUCKET\" region : \"\" #it can be empty Now just execute: kubectl create -f my-dataset.yaml -n { my-namespace }","title":"Create a Dataset for the S3 Bucket"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#provision-notebook-with-the-data-volume","text":"Now use the Kubeflow Central Dashboard to follow the rest of the guide. Choose the \"Notebook Servers\" item: Select \"New server\": Head over to the \"Data Volumes\" section and fill out the form as follows: Now you can press \"Launch\" to start the notebook server. After you connect, you can list the contents of /mnt/dataset and verity that the reflect the contents for your remote S3 bucket. NOTE : all the changes that you do in this directory (delete,create,modify) will be reflected on the remote bucket","title":"Provision Notebook with the Data Volume"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/","text":"We will show how you can use DLF to store/serve trained models using S3 Buckets. Requirements You have permissions in a namespace where you can use for kubeflow (to create TFJobs, deployments etc) Lets assume the namespace you can use is {my-namespace} . Feel free to change accordingly. Make sure you first follow the guide for Installation We will loosely follow the example posted in mnist_vanilla_k8s.ipynb NOTE: All example yaml files mentioned in the wiki are also available in examples/kubeflow Build model container There is a delta between existing distributed mnist examples and what's needed to run well as a TFJob. We will skip the kaniko part and just build and use the Dockerfile and model.py in examples/kubeflow cd examples/kubeflow docker build -t { MY-REGISTRY } /mnist-model -f Dockerfile.model . docker push { MY-REGISTRY } /mnist-model In case you use an authenticated registry, follow the instructions in configure-docker-credentials Create an S3 Bucket and its Dataset If you have an existing s3 bucket you can use, please proceed with this one. Otherwise follow the instructions in Configure IBM COS Storage Now we need to create a dataset to point to the newly created bucket. Create a file that looks like this: apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : your-dataset spec : local : type : \"COS\" accessKeyID : \"access_key_id\" secretAccessKey : \"secret_access_key\" endpoint : \"https://YOUR_ENDPOINT\" bucket : \"YOUR_BUCKET\" region : \"\" #it can be empty Now just execute: kubectl create -f my-dataset.yaml -n { my-namespace } Launch a TFJob Now we are ready to launch a tfjob in a much less verbose way since DLF takes care of mounting the dataset and providing access to the tensorflow pod: apiVersion : kubeflow.org/v1 kind : TFJob metadata : name : my-train spec : tfReplicaSpecs : Ps : replicas : 1 template : metadata : labels : dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" annotations : sidecar.istio.io/inject : \"false\" spec : serviceAccount : default-editor containers : - name : tensorflow command : - python - /opt/model.py - --tf-model-dir=/mnt/datasets/your-dataset/mnist - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export - --tf-train-steps=200 - --tf-batch-size=100 - --tf-learning-rate=0.1 image : yiannisgkoufas/mnist workingDir : /opt resources : limits : ephemeral-storage : \"10Gi\" requests : ephemeral-storage : \"10Gi\" restartPolicy : OnFailure Chief : replicas : 1 template : metadata : labels : dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" annotations : sidecar.istio.io/inject : \"false\" spec : serviceAccount : default-editor containers : - name : tensorflow resources : limits : ephemeral-storage : \"10Gi\" requests : ephemeral-storage : \"10Gi\" command : - python - /opt/model.py - --tf-model-dir=/mnt/datasets/your-dataset/mnist - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export - --tf-train-steps=200 - --tf-batch-size=100 - --tf-learning-rate=0.1 image : yiannisgkoufas/mnist restartPolicy : OnFailure Worker : replicas : 1 template : metadata : labels : dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" annotations : sidecar.istio.io/inject : \"false\" spec : serviceAccount : default-editor containers : - name : tensorflow command : - python - /opt/model.py - --tf-model-dir=/mnt/datasets/your-dataset/mnist - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export - --tf-train-steps=200 - --tf-batch-size=100 - --tf-learning-rate=0.1 image : yiannisgkoufas/mnist workingDir : /opt restartPolicy : OnFailure Make sure to replace your-dataset with the name of your dataset. Create the TFJob like that: kubectl create -f tfjob.yaml -n { my-namespace } You should see the job running and the model stored in the end in the remote S3 bucket. View the Model in Tensorboard You can inspect the model you created and stored in the remote S3 bucket by creating the following yaml file which again leverages the Dataset created. apiVersion : apps/v1 kind : Deployment metadata : labels : app : mnist-tensorboard name : mnist-tensorboard spec : selector : matchLabels : app : mnist-tensorboard template : metadata : labels : app : mnist-tensorboard version : v1 dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" annotations : sidecar.istio.io/inject : \"false\" spec : serviceAccount : default-editor containers : - command : - /usr/local/bin/tensorboard - --logdir=/mnt/datasets/your-dataset/mnist - --port=80 image : tensorflow/tensorflow:1.15.2-py3 name : tensorboard ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : labels : app : mnist-tensorboard name : mnist-tensorboard spec : ports : - name : http-tb port : 80 targetPort : 80 selector : app : mnist-tensorboard type : ClusterIP --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : mnist-tensorboard spec : gateways : - kubeflow/kubeflow-gateway hosts : - '*' http : - match : - uri : prefix : /mnist/default/tensorboard/ rewrite : uri : / route : - destination : host : mnist-tensorboard.default.svc.cluster.local port : number : 80 timeout : 300s Create the deployment: kubectl create -f tensorboard.yaml -n { my-namespace } You can expose the service and access it remotely as described here: Tensorboard access Model Serving Using KFServing You can leverage DLF to run the inference service on the model you trained using KFServing as follows: apiVersion : \"serving.kubeflow.org/v1alpha2\" kind : \"InferenceService\" metadata : name : \"mnist-sample\" spec : default : predictor : tensorflow : storageUri : \"pvc://your-dataset/mnist/export\" Create the yaml: kubectl create -f kfserving-inference.yaml -n { my-namespace } Model Serving Using Tensorflow Serving Again you can leverage DLF to serve the model you trained. apiVersion : apps/v1 kind : Deployment metadata : labels : app : mnist name : tensorflow-serving spec : selector : matchLabels : app : mnist-model template : metadata : annotations : sidecar.istio.io/inject : \"false\" labels : app : mnist-model version : v1 dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" spec : serviceAccount : default-editor containers : - args : - --port=9000 - --rest_api_port=8500 - --model_name=mnist - --model_base_path=/mnt/datasets/your-dataset/mnist/export command : - /usr/bin/tensorflow_model_server env : - name : modelBasePath value : /mnt/datasets/your-dataset/mnist/export image : tensorflow/serving:1.15.0 imagePullPolicy : IfNotPresent livenessProbe : initialDelaySeconds : 30 periodSeconds : 30 tcpSocket : port : 9000 name : mnist ports : - containerPort : 9000 - containerPort : 8500 resources : limits : cpu : \"4\" memory : 4Gi requests : cpu : \"1\" memory : 1Gi volumeMounts : - mountPath : /var/config/ name : model-config volumes : - configMap : name : tensorflow-serving name : model-config --- apiVersion : v1 kind : Service metadata : annotations : prometheus.io/path : /monitoring/prometheus/metrics prometheus.io/port : \"8500\" prometheus.io/scrape : \"true\" labels : app : mnist-model name : tensorflow-serving spec : ports : - name : grpc-tf-serving port : 9000 targetPort : 9000 - name : http-tf-serving port : 8500 targetPort : 8500 selector : app : mnist-model type : ClusterIP --- kind : ConfigMap apiVersion : v1 metadata : name : tensorflow-serving data : monitoring_config.txt : |- prometheus_config: {{ enable: true, path: \"/monitoring/prometheus/metrics\" }} Now create the deployment: kubectl create -f tensorflow-serving -n { my-namespace } If you want to deploy the demo with the MNIST UI follow the instructions in MNIST UI","title":"Model Storing and Serving"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#requirements","text":"You have permissions in a namespace where you can use for kubeflow (to create TFJobs, deployments etc) Lets assume the namespace you can use is {my-namespace} . Feel free to change accordingly. Make sure you first follow the guide for Installation We will loosely follow the example posted in mnist_vanilla_k8s.ipynb NOTE: All example yaml files mentioned in the wiki are also available in examples/kubeflow","title":"Requirements"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#build-model-container","text":"There is a delta between existing distributed mnist examples and what's needed to run well as a TFJob. We will skip the kaniko part and just build and use the Dockerfile and model.py in examples/kubeflow cd examples/kubeflow docker build -t { MY-REGISTRY } /mnist-model -f Dockerfile.model . docker push { MY-REGISTRY } /mnist-model In case you use an authenticated registry, follow the instructions in configure-docker-credentials","title":"Build model container"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#create-an-s3-bucket-and-its-dataset","text":"If you have an existing s3 bucket you can use, please proceed with this one. Otherwise follow the instructions in Configure IBM COS Storage Now we need to create a dataset to point to the newly created bucket. Create a file that looks like this: apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : your-dataset spec : local : type : \"COS\" accessKeyID : \"access_key_id\" secretAccessKey : \"secret_access_key\" endpoint : \"https://YOUR_ENDPOINT\" bucket : \"YOUR_BUCKET\" region : \"\" #it can be empty Now just execute: kubectl create -f my-dataset.yaml -n { my-namespace }","title":"Create an S3 Bucket and its Dataset"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#launch-a-tfjob","text":"Now we are ready to launch a tfjob in a much less verbose way since DLF takes care of mounting the dataset and providing access to the tensorflow pod: apiVersion : kubeflow.org/v1 kind : TFJob metadata : name : my-train spec : tfReplicaSpecs : Ps : replicas : 1 template : metadata : labels : dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" annotations : sidecar.istio.io/inject : \"false\" spec : serviceAccount : default-editor containers : - name : tensorflow command : - python - /opt/model.py - --tf-model-dir=/mnt/datasets/your-dataset/mnist - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export - --tf-train-steps=200 - --tf-batch-size=100 - --tf-learning-rate=0.1 image : yiannisgkoufas/mnist workingDir : /opt resources : limits : ephemeral-storage : \"10Gi\" requests : ephemeral-storage : \"10Gi\" restartPolicy : OnFailure Chief : replicas : 1 template : metadata : labels : dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" annotations : sidecar.istio.io/inject : \"false\" spec : serviceAccount : default-editor containers : - name : tensorflow resources : limits : ephemeral-storage : \"10Gi\" requests : ephemeral-storage : \"10Gi\" command : - python - /opt/model.py - --tf-model-dir=/mnt/datasets/your-dataset/mnist - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export - --tf-train-steps=200 - --tf-batch-size=100 - --tf-learning-rate=0.1 image : yiannisgkoufas/mnist restartPolicy : OnFailure Worker : replicas : 1 template : metadata : labels : dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" annotations : sidecar.istio.io/inject : \"false\" spec : serviceAccount : default-editor containers : - name : tensorflow command : - python - /opt/model.py - --tf-model-dir=/mnt/datasets/your-dataset/mnist - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export - --tf-train-steps=200 - --tf-batch-size=100 - --tf-learning-rate=0.1 image : yiannisgkoufas/mnist workingDir : /opt restartPolicy : OnFailure Make sure to replace your-dataset with the name of your dataset. Create the TFJob like that: kubectl create -f tfjob.yaml -n { my-namespace } You should see the job running and the model stored in the end in the remote S3 bucket.","title":"Launch a TFJob"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#view-the-model-in-tensorboard","text":"You can inspect the model you created and stored in the remote S3 bucket by creating the following yaml file which again leverages the Dataset created. apiVersion : apps/v1 kind : Deployment metadata : labels : app : mnist-tensorboard name : mnist-tensorboard spec : selector : matchLabels : app : mnist-tensorboard template : metadata : labels : app : mnist-tensorboard version : v1 dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" annotations : sidecar.istio.io/inject : \"false\" spec : serviceAccount : default-editor containers : - command : - /usr/local/bin/tensorboard - --logdir=/mnt/datasets/your-dataset/mnist - --port=80 image : tensorflow/tensorflow:1.15.2-py3 name : tensorboard ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : labels : app : mnist-tensorboard name : mnist-tensorboard spec : ports : - name : http-tb port : 80 targetPort : 80 selector : app : mnist-tensorboard type : ClusterIP --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : mnist-tensorboard spec : gateways : - kubeflow/kubeflow-gateway hosts : - '*' http : - match : - uri : prefix : /mnist/default/tensorboard/ rewrite : uri : / route : - destination : host : mnist-tensorboard.default.svc.cluster.local port : number : 80 timeout : 300s Create the deployment: kubectl create -f tensorboard.yaml -n { my-namespace } You can expose the service and access it remotely as described here: Tensorboard access","title":"View the Model in Tensorboard"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#model-serving-using-kfserving","text":"You can leverage DLF to run the inference service on the model you trained using KFServing as follows: apiVersion : \"serving.kubeflow.org/v1alpha2\" kind : \"InferenceService\" metadata : name : \"mnist-sample\" spec : default : predictor : tensorflow : storageUri : \"pvc://your-dataset/mnist/export\" Create the yaml: kubectl create -f kfserving-inference.yaml -n { my-namespace }","title":"Model Serving Using KFServing"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#model-serving-using-tensorflow-serving","text":"Again you can leverage DLF to serve the model you trained. apiVersion : apps/v1 kind : Deployment metadata : labels : app : mnist name : tensorflow-serving spec : selector : matchLabels : app : mnist-model template : metadata : annotations : sidecar.istio.io/inject : \"false\" labels : app : mnist-model version : v1 dataset.0.id : \"your-dataset\" dataset.0.useas : \"mount\" spec : serviceAccount : default-editor containers : - args : - --port=9000 - --rest_api_port=8500 - --model_name=mnist - --model_base_path=/mnt/datasets/your-dataset/mnist/export command : - /usr/bin/tensorflow_model_server env : - name : modelBasePath value : /mnt/datasets/your-dataset/mnist/export image : tensorflow/serving:1.15.0 imagePullPolicy : IfNotPresent livenessProbe : initialDelaySeconds : 30 periodSeconds : 30 tcpSocket : port : 9000 name : mnist ports : - containerPort : 9000 - containerPort : 8500 resources : limits : cpu : \"4\" memory : 4Gi requests : cpu : \"1\" memory : 1Gi volumeMounts : - mountPath : /var/config/ name : model-config volumes : - configMap : name : tensorflow-serving name : model-config --- apiVersion : v1 kind : Service metadata : annotations : prometheus.io/path : /monitoring/prometheus/metrics prometheus.io/port : \"8500\" prometheus.io/scrape : \"true\" labels : app : mnist-model name : tensorflow-serving spec : ports : - name : grpc-tf-serving port : 9000 targetPort : 9000 - name : http-tf-serving port : 8500 targetPort : 8500 selector : app : mnist-model type : ClusterIP --- kind : ConfigMap apiVersion : v1 metadata : name : tensorflow-serving data : monitoring_config.txt : |- prometheus_config: {{ enable: true, path: \"/monitoring/prometheus/metrics\" }} Now create the deployment: kubectl create -f tensorflow-serving -n { my-namespace } If you want to deploy the demo with the MNIST UI follow the instructions in MNIST UI","title":"Model Serving Using Tensorflow Serving"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/","text":"We will show how you can use DLF to provision Persistent Volume Claims via DLF so you can use it within Pipelines SDK. Requirements You have kubeflow installed and you can deploy pipelines using the Pipeline SDK. Make sure you first follow the guide for Installation We will just how you can adopt the examples located in contrib/volume_ops NOTE : For this guide you can use both an empty and pre-populated with data bucket. Example with creation of Dataset before the pipeline execution First you need to create a Dataset to point to the bucket you want to use. Create a file that looks like this: apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : your-dataset spec : local : type : \"COS\" accessKeyID : \"access_key_id\" secretAccessKey : \"secret_access_key\" endpoint : \"https://YOUR_ENDPOINT\" bucket : \"YOUR_BUCKET\" region : \"\" #it can be empty Now just execute: kubectl create -f my-dataset.yaml -n { my-namespace } Now within {my-namespace} you will find a PVC which you can use within your pipelines SDK without a problem. You can see the example below which can use the PVC which was created out of your dataset. import kfp import kfp.dsl as dsl from kfp.dsl import PipelineVolume @dsl . pipeline ( name = \"Volume Op DAG\" , description = \"The second example of the design doc.\" ) def volume_op_dag (): dataset = PipelineVolume ( \"your-dataset\" ) step1 = dsl . ContainerOp ( name = \"step1\" , image = \"library/bash:4.4.23\" , command = [ \"sh\" , \"-c\" ], arguments = [ \"echo 1|tee /data/file1\" ], pvolumes = { \"/data\" : dataset } ) step2 = dsl . ContainerOp ( name = \"step2\" , image = \"library/bash:4.4.23\" , command = [ \"sh\" , \"-c\" ], arguments = [ \"cp /data/file1 /data/file2\" ], pvolumes = { \"/data\" : step1 . pvolume } ) step3 = dsl . ContainerOp ( name = \"step3\" , image = \"library/bash:4.4.23\" , command = [ \"cat\" , \"/mnt/file1\" , \"/mnt/file2\" ], pvolumes = { \"/mnt\" : step2 . pvolume } ) if __name__ == \"__main__\" : import kfp.compiler as compiler compiler . Compiler () . compile ( volume_op_dag , __file__ + \".tar.gz\" ) Example with creation of Dataset as part of the pipeline execution If instead you want to create a Dataset as part of your pipeline, you can create the Dataset yaml and invoke a ResourceOp . Before that you need to make sure that the service account pipeline-runner in namespace kubeflow can create/delete Datasets, so make sure you execute kubectl apply -f examples/kubeflow/pipeline-runner-binding.yaml before running the pipeline. The example rolebinding definition is in examples/kubeflow/pipeline-runner-binding.yaml In the following pipeline we are creating the Dataset in step0 and then proceed to step1 to use it: import kfp.dsl as dsl import yaml from kfp.dsl import PipelineVolume # Make sure that you have applied ./pipeline-runner-binding.yaml # or any serviceAccount that should be allowed to create/delete datasets @dsl . pipeline ( name = \"Volume Op DAG\" , description = \"The second example of the design doc.\" ) def volume_op_dag (): datasetName = \"your-dataset\" dataset = PipelineVolume ( datasetName ) step0 = dsl . ResourceOp ( name = \"dataset-creation\" , k8s_resource = get_dataset_yaml ( datasetName , \"XXXXXXXXXXXXXXX\" , \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"http://your_endpoint.com\" , \"bucket-name\" , \"\" )) step1 = dsl . ContainerOp ( name = \"step1\" , image = \"library/bash:4.4.23\" , command = [ \"sh\" , \"-c\" ], arguments = [ \"echo 1|tee /data/file1\" ], pvolumes = { \"/data\" : dataset } ) . after ( step0 ) step2 = dsl . ContainerOp ( name = \"step2\" , image = \"library/bash:4.4.23\" , command = [ \"sh\" , \"-c\" ], arguments = [ \"cp /data/file1 /data/file2\" ], pvolumes = { \"/data\" : step1 . pvolume } ) step3 = dsl . ContainerOp ( name = \"step3\" , image = \"library/bash:4.4.23\" , command = [ \"cat\" , \"/mnt/file1\" , \"/mnt/file2\" ], pvolumes = { \"/mnt\" : step2 . pvolume } ) def get_dataset_yaml ( name , accessKey , secretAccessKey , endpoint , bucket , region ): print ( region ) dataset_spec = f \"\"\" apiVersion: com.ie.ibm.hpsys/v1alpha1 kind: Dataset metadata: name: { name } spec: local: type: \"COS\" accessKeyID: { accessKey } secretAccessKey: { secretAccessKey } endpoint: { endpoint } bucket: { bucket } region: { region } \"\"\" data = yaml . safe_load ( dataset_spec ) convert_none_to_str ( data ) return data","title":"PVCs for Pipelines SDK"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#requirements","text":"You have kubeflow installed and you can deploy pipelines using the Pipeline SDK. Make sure you first follow the guide for Installation We will just how you can adopt the examples located in contrib/volume_ops NOTE : For this guide you can use both an empty and pre-populated with data bucket.","title":"Requirements"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#example-with-creation-of-dataset-before-the-pipeline-execution","text":"First you need to create a Dataset to point to the bucket you want to use. Create a file that looks like this: apiVersion : com.ie.ibm.hpsys/v1alpha1 kind : Dataset metadata : name : your-dataset spec : local : type : \"COS\" accessKeyID : \"access_key_id\" secretAccessKey : \"secret_access_key\" endpoint : \"https://YOUR_ENDPOINT\" bucket : \"YOUR_BUCKET\" region : \"\" #it can be empty Now just execute: kubectl create -f my-dataset.yaml -n { my-namespace } Now within {my-namespace} you will find a PVC which you can use within your pipelines SDK without a problem. You can see the example below which can use the PVC which was created out of your dataset. import kfp import kfp.dsl as dsl from kfp.dsl import PipelineVolume @dsl . pipeline ( name = \"Volume Op DAG\" , description = \"The second example of the design doc.\" ) def volume_op_dag (): dataset = PipelineVolume ( \"your-dataset\" ) step1 = dsl . ContainerOp ( name = \"step1\" , image = \"library/bash:4.4.23\" , command = [ \"sh\" , \"-c\" ], arguments = [ \"echo 1|tee /data/file1\" ], pvolumes = { \"/data\" : dataset } ) step2 = dsl . ContainerOp ( name = \"step2\" , image = \"library/bash:4.4.23\" , command = [ \"sh\" , \"-c\" ], arguments = [ \"cp /data/file1 /data/file2\" ], pvolumes = { \"/data\" : step1 . pvolume } ) step3 = dsl . ContainerOp ( name = \"step3\" , image = \"library/bash:4.4.23\" , command = [ \"cat\" , \"/mnt/file1\" , \"/mnt/file2\" ], pvolumes = { \"/mnt\" : step2 . pvolume } ) if __name__ == \"__main__\" : import kfp.compiler as compiler compiler . Compiler () . compile ( volume_op_dag , __file__ + \".tar.gz\" )","title":"Example with creation of Dataset before the pipeline execution"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#example-with-creation-of-dataset-as-part-of-the-pipeline-execution","text":"If instead you want to create a Dataset as part of your pipeline, you can create the Dataset yaml and invoke a ResourceOp . Before that you need to make sure that the service account pipeline-runner in namespace kubeflow can create/delete Datasets, so make sure you execute kubectl apply -f examples/kubeflow/pipeline-runner-binding.yaml before running the pipeline. The example rolebinding definition is in examples/kubeflow/pipeline-runner-binding.yaml In the following pipeline we are creating the Dataset in step0 and then proceed to step1 to use it: import kfp.dsl as dsl import yaml from kfp.dsl import PipelineVolume # Make sure that you have applied ./pipeline-runner-binding.yaml # or any serviceAccount that should be allowed to create/delete datasets @dsl . pipeline ( name = \"Volume Op DAG\" , description = \"The second example of the design doc.\" ) def volume_op_dag (): datasetName = \"your-dataset\" dataset = PipelineVolume ( datasetName ) step0 = dsl . ResourceOp ( name = \"dataset-creation\" , k8s_resource = get_dataset_yaml ( datasetName , \"XXXXXXXXXXXXXXX\" , \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" , \"http://your_endpoint.com\" , \"bucket-name\" , \"\" )) step1 = dsl . ContainerOp ( name = \"step1\" , image = \"library/bash:4.4.23\" , command = [ \"sh\" , \"-c\" ], arguments = [ \"echo 1|tee /data/file1\" ], pvolumes = { \"/data\" : dataset } ) . after ( step0 ) step2 = dsl . ContainerOp ( name = \"step2\" , image = \"library/bash:4.4.23\" , command = [ \"sh\" , \"-c\" ], arguments = [ \"cp /data/file1 /data/file2\" ], pvolumes = { \"/data\" : step1 . pvolume } ) step3 = dsl . ContainerOp ( name = \"step3\" , image = \"library/bash:4.4.23\" , command = [ \"cat\" , \"/mnt/file1\" , \"/mnt/file2\" ], pvolumes = { \"/mnt\" : step2 . pvolume } ) def get_dataset_yaml ( name , accessKey , secretAccessKey , endpoint , bucket , region ): print ( region ) dataset_spec = f \"\"\" apiVersion: com.ie.ibm.hpsys/v1alpha1 kind: Dataset metadata: name: { name } spec: local: type: \"COS\" accessKeyID: { accessKey } secretAccessKey: { secretAccessKey } endpoint: { endpoint } bucket: { bucket } region: { region } \"\"\" data = yaml . safe_load ( dataset_spec ) convert_none_to_str ( data ) return data","title":"Example with creation of Dataset as part of the pipeline execution"}]}